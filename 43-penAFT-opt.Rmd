---
title: "penAFT: Optimal hyperparameters"
author: "Anonymous"
date: "August 1st, 2022"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{penAFT: Optimal hyperparameters}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE, echo=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  results = "markup",
  highlight = TRUE,
  fig.width = 12, fig.height=8, fig.path='Figs/'
)
```

# R session setup

```{r Rsetup-ls, include=FALSE}
rm (list=ls())
```

```{r Rsetup}
library(cvaUtils)
library(penAFT)
sessionInfo()
```

```{r input-info, include=FALSE}
vignette_nm  <- "43-penAFT-opt"
vignette_Rmd <- paste(vignette_nm,".Rmd", sep = "")
```

* This is `r vignette_nm` vignette that illustrates how to inspect `cva` object
* Rmd file  `r vignette_Rmd` was executed at `r Sys.time()`

```{r load-Rdata}
load(file = "Rdata/42-penAFT-cva.Rdata")
ls()
```


```{r cva-lossgrid}
loss_grid <- lossgrid(penAFTcva.en)
names(loss_grid)
save(loss_grid, file = "Rdata/43-penAFT-loss-grid.Rmd
```


```{r cva-lossgrid}
# loss_grid_lambda_min <- loss_grid[, "lambda.min"] 
# loss_grid[loss_grid_lambda_min, ]
loss_grid_cv.err.linPred_min <-loss_grid[,"cv.err.linPred_min"]
loss_grid[loss_grid_cv.err.linPred_min, ]

knitr::knit_exit()
# minlossplot(penAFTcva.en)

```

Non zero coefficients of penAFT fit

```{r penAFT-test-nonzero-coefs}
coef.en.10_nz <- coef.en.10$beta[coef.en.10$beta !=0]
names(coef.en.10_nz) <- rownames(coef.en.10$beta[coef.en.10$beta !=0])
coef.en.10_nz

Check whether X and Y are properly defined.

```{r chkdata}
is.matrix(X) # X needs to be a matrix
knitr::knit_exit()
```

# Find Optimal values of hyperparameters

Goal: Find optimal values of alpha and lambda hyperparameters using C-V.
https://stats.stackexchange.com/questions/254612/how-to-obtain-optimal-hyperparameters-after-nested-cross-validation

* Use different values of `alpha` and store in `alphx` 
* Note: if alpha closer to 0 then more variables selected into the model, and more coefficients are negative
* Code in this section corresponds to 

## Penalized logistic regression 

* Load and attach libraries
* Assign K: the number of iterations in the outer loop
* Assign J: the number of iterations in the inner loop
* Create K folds

```{r setup2}
```
* https://cran.r-project.org/web/packages/glmnetUtils/vignettes/intro.html

# cva.glmnet. nested cross-validation


Nested cross-validation is discussed in more detail in:

* [nested-cross-validation-for-model-selection](https://stats.stackexchange.com/questions/65128/nested-cross-validation-for-model-selection)
* [build-the-final-model-and-tune-probability-threshold-after-nested-C-V](https://stats.stackexchange.com/questions/232897/how-to-build-the-final-model-and-tune-probability-threshold-after-nested-cross-v/233027#233027)
* [training-on-the-full-dataset-after-cross-validation](https://stats.stackexchange.com/questions/11602/training-on-the-full-dataset-after-cross-validation)
* {this document](https://stuartlacy.co.uk/2016/02/04/how-to-correctly-use-cross-validation-in-predictive-modelling/)


* K-fold C-V to evaluate model
* J-fold C-V used to tune hyper-parameters
* For the sake of presentation simplicity assume that n=1000

```{r initloop}

K <- 2     # Number of iterations in the outer loop
J <- 5     # Number of iterations in the inner loop
alphx <- seq(0, 1, len = 11)^3   # 0-ridge, 1-LASSO
alphx_nms <- paste("a", alphx, sep ="")
## fldsK <- createFolds(Y, k = K, list = TRUE, returnTrain = FALSE)
set.seed(1234)
nrowdtc   <-  nrow(dtc)
fldsK <- sample(1:K, size = nrowdtc, replace = TRUE)

```

Initialize objects to be created in the main loop.

```{r looplists}
cvafitK <- list()
```

Main loop for nested cross-validation.

```{r mainloop}
message("--->>> Outer loop K = ", K,"\n")
# For each outer fold 1..k do:
   for (k in 1:K){

   # Combine the data from folds != k into a training set of 900 and test data of 100
      train_k <- which(fldsK !=k)
       Xo_k <- X[train_k,] # Train data
       Yo_k <- Y[train_k]  
       Xp_k <- X[-train_k,] # Test data
       Yp_k <- Y[-train_k] 
 
    
      message("====>>>>> k =", k,"/",K,", outer split (", nrow(Xo_k), "+", nrow(Xp_k), ")/", length(fldsK)) 

      # CV with the same folds for different values of alpha
       cva.foldid <- sample(1:J, size = length(Yo_k), replace = TRUE)
       cvafitk    <- lapply(alphx, FUN = function(x){
                               res <-cv.glmnet(Xo_k,Yo_k, alpha=x, family="binomial", foldid = cva.foldid)
                               return(res)
                        }) 
       names(cvafitk) <- alphx_nms 
       cvafitK[[length(cvafitK) + 1]] <- cvafitk 
  message(" - assesing performnce of cvafitk model in Xp_k data (n =", nrow(Xp_k), ")")
   } # for k 
   names(cvafitK) <- paste("k", 1:K, sep ="")
```

Check the contents of the created object `cvafitK` and save results from the main loop in an external file.

```{r mainres}
length(cvafitK)            # Should be equal to K
class(res1 <- cvafitK$k1$a0.5)
str(res1)
res1$lambda.min
save(cvafitK, file = "cvafitK.Rdata")
# knitr::knit_exit()
```

* # Assess performance of the fitted models using test data (from k-th fold)

# Build a final model

* Use cross-validation on the _whole_ set to choose the hyper-parameters 

* Below we create a list of `cv.glmnet` objects is created

```{r final model}
cva.foldid <- sample(1:J, size = length(Y), replace = TRUE) 

cva_full <- cva.glmnet(X, Y, family = "binomial", alpha = alphx, nfolds = J, foldid = cva.foldid)                     
str(cva_full)
``` 

Get params from a  `cv.glmnet` object. Compome

```{r get_cvlistparms}
get_cvafit_params <- function(fit) {
  alpha <- fit$alpha
  lambdaMin   <- sapply(fit$modlist, `[[`, "lambda.min")
  lambda1SE   <- sapply(fit$modlist, `[[`, "lambda.1se")
  min_mod_cvm <- sapply(fit$modlist, function(mod) {min(mod$cvm)})
  optm        <- min_mod_cvm == min(min_mod_cvm) 
  data.frame(alpha = alpha, 
             lambdaMin = lambdaMin,
             lambda1SE = lambda1SE,
             min_mod_cvm = min_mod_cvm, 
             optm=optm)
}
(cvafit_params <- get_cvafit_params(cva_full))
```

Extract optimal parameter values.

```{r optimum-params-for-cva-full-model}
cvafit_opt_params <- as.matrix(subset(cvafit_params, optm == TRUE, select =c(alpha,lambda1SE)))
print(cvafit_opt_params)
alpha_opt <- cvafit_opt_params[, "alpha"]
lmbda_opt <- cvafit_opt_params[, "lambda1SE"]
```

Run glmnet with optimal alpha and lambda

```{r glmnet-final}
fit_fnl <- glmnet(X, Y, family = "binomial", alpha = alpha_opt, lambda= lmbda_opt)
coef(fit_fnl)
```

# Performance of the final model

In this section, we _emulate_ external validation of the final model. 


## Create `new_dtc` dataset

First, we create an auxiliary `dtc_new` data frame to assess performance of the final model. It will be used
as a proxy of an external dataset (for example Joslin and Steno)

```{r dtc-new}
set.seed(1122)
nx  <- nrow(dtc)
ns  <- floor(0.9*nx) # 90pct of teh total sample
idx <- sample(1:nx, size =ns)
dtc_new <- dtc[idx,]
class(dtc_new)
## knitr::knit_exit()
```
Data frame `dtc_new_` contains `r nrow(dtc_new)` rows and `r nx` in `dtc` dataset. 


```{r XY-mtx-new}
X_new <- as.matrix(dtc_new[, 2:25])
Y_new <- dtc_new[, 1:1] # status 
n <- length(Y_new)
```




